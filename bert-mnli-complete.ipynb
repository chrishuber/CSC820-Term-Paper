{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91ac1a8",
   "metadata": {},
   "source": [
    "<b>Chris Huber\\\n",
    "    CSC820, Prof. Anagha Kulkari\n",
    "    Spring 2020<b>\n",
    "    \n",
    "<p>This notebook shows how to train the BERT model on new data and run it to create inference predictions of 0, 1, or 2 which correspond to contradiction, netural, or entailment on a test set of data that was submitted for a Kaggle competition. This is based on another notebook used for the Contradictory, Dear Watson TPU competition by a user named Marsh but adapted for GPU/CUDA processing. It requires a CUDA-enabled computer with an appropriate GPU in order to run. It uses a PyTorch neural network to learn the labels in the train data, uses a validation set to improve the model, and then outputs inference predictions for a test set using that model.</p>\n",
    "    \n",
    "<p> My results came out at 0.83738 F1 score indicating a far high degree of accuracy than could be achieved by randomly selecting classes which tells us that the experiment was a success. By altering the model or adding/changing features the score should be able to be improved upon.</p>\n",
    "    \n",
    "<p>NB: This notebook took approximately 3 days to fully train on the 392702 rows in the train set on a gaming PC, so if you use your own machine be prepared to have it tied up for a while and make sure to turn sleep mode off.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8c87c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration chrishuber--kaggle_mnli-df90bb2b9c35e99b\n",
      "Reusing dataset json (C:\\Users\\chris\\.cache\\huggingface\\datasets\\json\\chrishuber--kaggle_mnli-df90bb2b9c35e99b\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f619ef5937346cda0e9047614e71682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID', 'sentence1', 'sentence1_binary_parse', 'sentence1_parse', 'sentence2', 'sentence2_binary_parse', 'sentence2_parse'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID', 'sentence1', 'sentence1_binary_parse', 'sentence1_parse', 'sentence2', 'sentence2_binary_parse', 'sentence2_parse'],\n",
       "        num_rows: 19643\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID', 'sentence1', 'sentence1_binary_parse', 'sentence1_parse', 'sentence2', 'sentence2_binary_parse', 'sentence2_parse'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaForCausalLM, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# I created a dataset for this project which includes the MNLI 1.0 train dataset, the Kaggle dev dataset for the \n",
    "# Matched MulitNLI Competition (https://www.kaggle.com/competitions/multinli-matched-open-evaluation) and an \n",
    "# unlabelled test set. \n",
    "raw_datasets = load_dataset(\"chrishuber/kaggle_mnli\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b5dcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392702, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = raw_datasets['train'].to_pandas()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd07b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = raw_datasets['validation'].to_pandas()\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a4b80eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[hidden, hidden, hidden, hidden, hidden]</td>\n",
       "      <td>slate</td>\n",
       "      <td>hidden</td>\n",
       "      <td>9847</td>\n",
       "      <td>9847</td>\n",
       "      <td>That which binds together Chinese.</td>\n",
       "      <td>( ( That ( which ( binds ( together Chinese ) ...</td>\n",
       "      <td>(ROOT (FRAG (NP (NP (DT That)) (SBAR (WHNP (WD...</td>\n",
       "      <td>This is a shared value among Chinese people.</td>\n",
       "      <td>( This ( ( is ( ( a ( shared value ) ) ( among...</td>\n",
       "      <td>(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (NP (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hidden, hidden, hidden, hidden, hidden]</td>\n",
       "      <td>government</td>\n",
       "      <td>hidden</td>\n",
       "      <td>9848</td>\n",
       "      <td>9848</td>\n",
       "      <td>The actual length of an individual worker's H-...</td>\n",
       "      <td>( ( ( The ( actual length ) ) ( of ( ( an ( in...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT The) (JJ actual) (NN leng...</td>\n",
       "      <td>The location of the employer effects the lengt...</td>\n",
       "      <td>( ( ( The location ) ( of ( the employer ) ) )...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT The) (NN location)) (PP (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hidden, hidden, hidden, hidden, hidden]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>hidden</td>\n",
       "      <td>9849</td>\n",
       "      <td>9849</td>\n",
       "      <td>Every man I put down left me empty.</td>\n",
       "      <td>( ( ( Every man ) ( I ( put down ) ) ) ( ( lef...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT Every) (NN man)) (SBAR (S...</td>\n",
       "      <td>I felt empty after every man I put down.</td>\n",
       "      <td>( I ( ( ( felt ( empty ( after ( every man ) )...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBD felt) (ADJP (JJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[hidden, hidden, hidden, hidden, hidden]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>hidden</td>\n",
       "      <td>9850</td>\n",
       "      <td>9850</td>\n",
       "      <td>and uh i really think that if uh like after se...</td>\n",
       "      <td>( and ( ( uh i ) ( really ( think ( ( that if ...</td>\n",
       "      <td>(ROOT (FRAG (CC and) (NP (NP (FW uh) (FW i)) (...</td>\n",
       "      <td>Women wouldn't have gone to work after the sec...</td>\n",
       "      <td>( Women ( ( ( would n't ) ( have ( gone ( to (...</td>\n",
       "      <td>(ROOT (S (NP (NNP Women)) (VP (MD would) (RB n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hidden, hidden, hidden, hidden, hidden]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>hidden</td>\n",
       "      <td>9851</td>\n",
       "      <td>9851</td>\n",
       "      <td>yep yeah yeah it was i ended up going into ban...</td>\n",
       "      <td>( ( yep yeah ) ( yeah ( ( ( ( it ( ( was i ) (...</td>\n",
       "      <td>(ROOT (S (NP (NN yep) (NN yeah)) (VP (VBP yeah...</td>\n",
       "      <td>I have no idea what bankruptcy is like.</td>\n",
       "      <td>( I ( ( have ( ( no idea ) ( what ( bankruptcy...</td>\n",
       "      <td>(ROOT (S (NP (PRP I)) (VP (VBP have) (NP (NP (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           annotator_labels       genre gold_label  pairID  \\\n",
       "0  [hidden, hidden, hidden, hidden, hidden]       slate     hidden    9847   \n",
       "1  [hidden, hidden, hidden, hidden, hidden]  government     hidden    9848   \n",
       "2  [hidden, hidden, hidden, hidden, hidden]     fiction     hidden    9849   \n",
       "3  [hidden, hidden, hidden, hidden, hidden]   telephone     hidden    9850   \n",
       "4  [hidden, hidden, hidden, hidden, hidden]   telephone     hidden    9851   \n",
       "\n",
       "   promptID                                          sentence1  \\\n",
       "0      9847                 That which binds together Chinese.   \n",
       "1      9848  The actual length of an individual worker's H-...   \n",
       "2      9849                Every man I put down left me empty.   \n",
       "3      9850  and uh i really think that if uh like after se...   \n",
       "4      9851  yep yeah yeah it was i ended up going into ban...   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( That ( which ( binds ( together Chinese ) ...   \n",
       "1  ( ( ( The ( actual length ) ) ( of ( ( an ( in...   \n",
       "2  ( ( ( Every man ) ( I ( put down ) ) ) ( ( lef...   \n",
       "3  ( and ( ( uh i ) ( really ( think ( ( that if ...   \n",
       "4  ( ( yep yeah ) ( yeah ( ( ( ( it ( ( was i ) (...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (FRAG (NP (NP (DT That)) (SBAR (WHNP (WD...   \n",
       "1  (ROOT (S (NP (NP (DT The) (JJ actual) (NN leng...   \n",
       "2  (ROOT (S (NP (NP (DT Every) (NN man)) (SBAR (S...   \n",
       "3  (ROOT (FRAG (CC and) (NP (NP (FW uh) (FW i)) (...   \n",
       "4  (ROOT (S (NP (NN yep) (NN yeah)) (VP (VBP yeah...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0       This is a shared value among Chinese people.   \n",
       "1  The location of the employer effects the lengt...   \n",
       "2           I felt empty after every man I put down.   \n",
       "3  Women wouldn't have gone to work after the sec...   \n",
       "4           I have no idea what bankruptcy is like.    \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( This ( ( is ( ( a ( shared value ) ) ( among...   \n",
       "1  ( ( ( The location ) ( of ( the employer ) ) )...   \n",
       "2  ( I ( ( ( felt ( empty ( after ( every man ) )...   \n",
       "3  ( Women ( ( ( would n't ) ( have ( gone ( to (...   \n",
       "4  ( I ( ( have ( ( no idea ) ( what ( bankruptcy...   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (DT This)) (VP (VBZ is) (NP (NP (...  \n",
       "1  (ROOT (S (NP (NP (DT The) (NN location)) (PP (...  \n",
       "2  (ROOT (S (NP (PRP I)) (VP (VBD felt) (ADJP (JJ...  \n",
       "3  (ROOT (S (NP (NNP Women)) (VP (MD would) (RB n...  \n",
       "4  (ROOT (S (NP (PRP I)) (VP (VBP have) (NP (NP (...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the incorrect test set, should be 9796 rows\n",
    "# df_test = raw_datasets['test'].to_pandas()\n",
    "# df_test.shape\n",
    "\n",
    "# Unfortunately, the data that I uploaded to my Huggingface.co repo for the test set is not the correct one so I had to\n",
    "# re-import it locally from a JSON file\n",
    "df_test = pd.read_json(\"./multinli_0.9_test_matched_unlabeled.jsonl\", lines=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3ddc100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9796"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0bf25c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"premise\"] = df_test[\"sentence1\"]\n",
    "df_test[\"hypothesis\"] = df_test[\"sentence2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eaeb67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### truncating datasets for testing\n",
    "# df_train = df_train.head(3900)\n",
    "# df_val = df_val.head(200)\n",
    "# df_test = df_test.head(200)\n",
    "# df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e5f67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(label):\n",
    "    if label == \"neutral\":\n",
    "        return 1\n",
    "    elif label == \"contradiction\":\n",
    "        return 0\n",
    "    elif label == \"entailment\":\n",
    "        return 2\n",
    "    \n",
    "df_train[\"labels\"] = df_train[\"gold_label\"].apply(lambda x: convert_to_int(x))\n",
    "df_val[\"labels\"] = df_val[\"gold_label\"].apply(lambda x: convert_to_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf78046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"premise\"] = df_train[\"sentence1\"]\n",
    "df_train[\"hypothesis\"] = df_train[\"sentence2\"]\n",
    "df_val[\"premise\"] = df_val[\"sentence1\"]\n",
    "df_val[\"hypothesis\"] = df_val[\"sentence2\"]\n",
    "df_test[\"premise\"] = df_test[\"sentence1\"]\n",
    "df_test[\"hypothesis\"] = df_test[\"sentence2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9409bded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "      <th>labels</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>31193n</td>\n",
       "      <td>31193</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>( ( Conceptually ( cream skimming ) ) ( ( has ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>( ( ( Product and ) geography ) ( ( are ( what...</td>\n",
       "      <td>(ROOT (S (NP (NN Product) (CC and) (NN geograp...</td>\n",
       "      <td>1</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>101457e</td>\n",
       "      <td>101457</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>( you ( ( know ( during ( ( ( the season ) and...</td>\n",
       "      <td>(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>( You ( ( ( ( lose ( the things ) ) ( to ( the...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...</td>\n",
       "      <td>2</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>134793e</td>\n",
       "      <td>134793</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>( ( One ( of ( our number ) ) ) ( ( will ( ( (...</td>\n",
       "      <td>(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>( ( ( A member ) ( of ( my team ) ) ) ( ( will...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...</td>\n",
       "      <td>2</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>37397e</td>\n",
       "      <td>37397</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...</td>\n",
       "      <td>(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>( ( This information ) ( ( belongs ( to them )...</td>\n",
       "      <td>(ROOT (S (NP (DT This) (NN information)) (VP (...</td>\n",
       "      <td>2</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>50563n</td>\n",
       "      <td>50563</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>( yeah ( i ( ( tell you ) ( what ( ( though ( ...</td>\n",
       "      <td>(ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>( ( The ( tennis shoes ) ) ( ( have ( ( a rang...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels       genre  gold_label   pairID promptID  \\\n",
       "0        [neutral]  government     neutral   31193n    31193   \n",
       "1     [entailment]   telephone  entailment  101457e   101457   \n",
       "2     [entailment]     fiction  entailment  134793e   134793   \n",
       "3     [entailment]     fiction  entailment   37397e    37397   \n",
       "4        [neutral]   telephone     neutral   50563n    50563   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  Conceptually cream skimming has two basic dime...   \n",
       "1  you know during the season and i guess at at y...   \n",
       "2  One of our number will carry out your instruct...   \n",
       "3  How do you know? All this is their information...   \n",
       "4  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( Conceptually ( cream skimming ) ) ( ( has ...   \n",
       "1  ( you ( ( know ( during ( ( ( the season ) and...   \n",
       "2  ( ( One ( of ( our number ) ) ) ( ( will ( ( (...   \n",
       "3  ( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...   \n",
       "4  ( yeah ( i ( ( tell you ) ( what ( ( though ( ...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...   \n",
       "1  (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...   \n",
       "2  (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...   \n",
       "3  (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...   \n",
       "4  (ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( ( Product and ) geography ) ( ( are ( what...   \n",
       "1  ( You ( ( ( ( lose ( the things ) ) ( to ( the...   \n",
       "2  ( ( ( A member ) ( of ( my team ) ) ) ( ( will...   \n",
       "3  ( ( This information ) ( ( belongs ( to them )...   \n",
       "4  ( ( The ( tennis shoes ) ) ( ( have ( ( a rang...   \n",
       "\n",
       "                                     sentence2_parse  labels  \\\n",
       "0  (ROOT (S (NP (NN Product) (CC and) (NN geograp...       1   \n",
       "1  (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...       2   \n",
       "2  (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...       2   \n",
       "3  (ROOT (S (NP (DT This) (NN information)) (VP (...       2   \n",
       "4  (ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...       1   \n",
       "\n",
       "                                             premise  \\\n",
       "0  Conceptually cream skimming has two basic dime...   \n",
       "1  you know during the season and i guess at at y...   \n",
       "2  One of our number will carry out your instruct...   \n",
       "3  How do you know? All this is their information...   \n",
       "4  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                                          hypothesis  \n",
       "0  Product and geography are what make cream skim...  \n",
       "1  You lose the things to the following level if ...  \n",
       "2  A member of my team will execute your orders w...  \n",
       "3                  This information belongs to them.  \n",
       "4           The tennis shoes have a range of prices.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc336a0",
   "metadata": {},
   "source": [
    "<h3>Using BERT base uncased as our training model. I had to reduce the max_len to 256 to prevent CUDA from giving me an out-of-memory error which I think affected the accuracy since many of the passages were quite long.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "edff35dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_TYPE = 'bert-base-uncased'\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Saving 5 TPU models will exceed the 4.9GB disk space.\n",
    "# Therefore, will will only train on 3 folds.\n",
    "NUM_FOLDS_TO_TRAIN = 3\n",
    "\n",
    "L_RATE = 1e-5\n",
    "MAX_LEN = 256\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "NUM_CORES = os.cpu_count()\n",
    "\n",
    "NUM_CORES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faaebb",
   "metadata": {},
   "source": [
    "<h3>Verify that CUDA is enabled and being used as the device.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37cc8019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef815d",
   "metadata": {},
   "source": [
    "<h3>We need to use the BertTokeizer with the BERT model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b174a0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc04062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df_data = df\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get the sentences to compare from the dataframe\n",
    "        premise = self.df_data.loc[index, 'sentence1']\n",
    "        hypothesis = self.df_data.loc[index, 'sentence2']\n",
    "        # optional output if you want to watch it run\n",
    "        # print(sentence1)\n",
    "        # print(sentence2)\n",
    " \n",
    "        # Process the sentence\n",
    "        # ---------------------\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                    premise, hypothesis,           # Sentences to encode.\n",
    "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
    "               )  \n",
    "        \n",
    "        # These are torch tensors already.\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "        token_type_ids = encoded_dict['token_type_ids'][0]\n",
    "        \n",
    "        # Convert the target to a torch tensor\n",
    "        target = torch.tensor(self.df_data.loc[index, 'labels'])\n",
    "        target = target.type(torch.LongTensor)\n",
    "        \n",
    "        sample = (padded_token_list, att_mask, token_type_ids, target)\n",
    "\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df_data = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get the sentence from the dataframe\n",
    "        sentence1 = self.df_data.loc[index, 'sentence1']\n",
    "        sentence2 = self.df_data.loc[index, 'sentence2']\n",
    "\n",
    "        # Process the sentence\n",
    "        # ---------------------\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                    sentence1, sentence2,           # Sentence to encode.\n",
    "                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
    "               )\n",
    "        \n",
    "        # These are torch tensors already.\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "        token_type_ids = encoded_dict['token_type_ids'][0]\n",
    "               \n",
    "        sample = (padded_token_list, att_mask, token_type_ids)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba89307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f62dca7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID',\n",
       "       'sentence1', 'sentence1_binary_parse', 'sentence1_parse', 'sentence2',\n",
       "       'sentence2_binary_parse', 'sentence2_parse', 'labels', 'premise',\n",
       "       'hypothesis'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b531837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98176\n",
      "5000\n",
      "4911\n"
     ]
    }
   ],
   "source": [
    "train_data = CompDataset(df_train)\n",
    "val_data = CompDataset(df_val)\n",
    "test_data = TestDataset(df_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=0)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=0)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                       num_workers=0)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d1482b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get one train batch\n",
    "padded_token_list, att_mask, token_type_ids, target = next(iter(train_dataloader))\n",
    "\n",
    "print(padded_token_list.shape)\n",
    "print(att_mask.shape)\n",
    "print(token_type_ids.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a5042ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "torch.Size([4, 256])\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Get one test batch\n",
    "padded_token_list, att_mask, token_type_ids = next(iter(test_dataloader))\n",
    "\n",
    "print(padded_token_list.shape)\n",
    "print(att_mask.shape)\n",
    "print(token_type_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d65100",
   "metadata": {},
   "source": [
    "<h3>We are using BERTForSequenceClassification here which is the correct one to produce classification-based results.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86da43a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:143: UserWarning: \n",
      "NVIDIA GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_TYPE, \n",
    "    num_labels = 3, \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False)\n",
    "\n",
    "# Send the model to the device.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98eafa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=8,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=0)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "b_input_ids = batch[0].to(device)\n",
    "b_input_mask = batch[1].to(device)\n",
    "b_token_type_ids = batch[2].to(device)\n",
    "b_labels = batch[3].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a02e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(b_input_ids, \n",
    "                token_type_ids=b_token_type_ids, \n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bbdcfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.0508, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.7317, -0.5870, -0.2513],\n",
       "        [-0.4535, -0.4830, -0.2485],\n",
       "        [-0.7164, -0.5949, -0.2626],\n",
       "        [-0.5369, -0.4946, -0.2705],\n",
       "        [-0.7294, -0.5723, -0.2729],\n",
       "        [-0.5691, -0.5592, -0.2260],\n",
       "        [-0.6258, -0.5219, -0.2485],\n",
       "        [-0.6047, -0.4873, -0.3121]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d00dc823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output is a tuple: (loss, preds)\n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e7f467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0508, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the loss.\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa0f646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7317, -0.5870, -0.2513],\n",
       "        [-0.4535, -0.4830, -0.2485],\n",
       "        [-0.7164, -0.5949, -0.2626],\n",
       "        [-0.5369, -0.4946, -0.2705],\n",
       "        [-0.7294, -0.5723, -0.2729],\n",
       "        [-0.5691, -0.5592, -0.2260],\n",
       "        [-0.6258, -0.5219, -0.2485],\n",
       "        [-0.6047, -0.4873, -0.3121]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the predictions.\n",
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4c628c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = outputs[1].detach().cpu().numpy()\n",
    "\n",
    "y_true = b_labels.detach().cpu().numpy()\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a750242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da499cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# This is the accuracy without any fine tuning.\n",
    "val_acc = accuracy_score(y_true, y_pred)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d59771c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# The loss and preds are Torch tensors\n",
    "print(type(outputs[0]))\n",
    "print(type(outputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "559ca8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_train\n",
    "val_df = df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0dd45d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num folds used for training: 3\n",
      "======== Epoch 1 / 1 ========\n",
      "\n",
      "== Fold Model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Batch 1 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1682 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6461 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7814 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10296 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11083 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12054 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13193 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13745 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14816 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15937 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17928 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21316 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25968 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 29546 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 29777 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32534 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33049 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33524 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34766 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36577 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37960 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 38942 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39543 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39766 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40821 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44137 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44261 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44441 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 47961 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49427 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49479 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49698 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49836 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49879 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50855 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51792 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 52725 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 53263 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55274 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55690 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58029 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58584 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60174 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 61276 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 61797 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 62635 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 63506 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64689 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65284 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 67896 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 70003 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 73188 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 75512 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 76271 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 78224 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80028 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80051 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 81286 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 85917 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 86389 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89894 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90644 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 91436 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 92644 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 93465 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 94981 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 95495 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 95686 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 62182.2570842671\n",
      "\n",
      "Validation...\n",
      "Val loss: 3278.731503564166\n",
      "Val acc:  0.81405\n",
      "Saved model as  model_0.bin\n",
      "\n",
      "== Fold Model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Batch 509 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2793 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3073 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3491 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3614 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3826 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4819 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7688 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9426 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10430 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12266 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15007 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17907 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18410 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18883 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18906 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24374 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24600 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25106 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25820 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 26243 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28888 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30744 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31865 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32049 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32473 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32510 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 38326 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39137 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 43618 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 43677 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44136 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44789 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50045 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50801 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54043 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55310 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55501 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58006 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58231 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58416 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 59064 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 59573 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65541 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65726 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 66507 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 68430 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 68966 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 71095 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 71956 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 73536 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 75952 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 78310 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79094 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79913 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80462 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 81470 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 82298 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 85356 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 86821 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 88230 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 88260 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 88433 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89558 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89964 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90346 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90601 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 93713 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 95067 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 63010.6075701816\n",
      "\n",
      "Validation...\n",
      "Val loss: 2971.0408669109456\n",
      "Val acc:  0.80035\n",
      "Saved model as  model_1.bin\n",
      "\n",
      "== Fold Model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Batch 474 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4301 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5294 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9967 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10321 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11980 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12269 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12441 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13086 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13511 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13971 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15640 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18246 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18306 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19262 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20180 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21667 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21991 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22851 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23081 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25464 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30946 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 31260 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33824 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37731 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40032 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 42601 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 46562 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 48986 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49018 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50652 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51024 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 52490 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54412 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54950 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55375 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55401 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 57771 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 58620 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 61720 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 63423 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 63485 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 65130 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 67410 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 69740 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 71642 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 74738 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 75306 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 76277 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80428 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 81083 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 81771 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 83433 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 83827 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 85936 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 86457 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 87629 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 88696 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89089 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89436 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89934 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89935 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90727 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 91141 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 91520 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 92228 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 95832 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 97208 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 97599 of 98176\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 61690.020144118695\n",
      "\n",
      "Validation...\n",
      "Val loss: 3333.2564018784324\n",
      "Val acc:  0.81315\n",
      "Saved model as  model_2.bin\n",
      "\n",
      "CV Acc: 0.8091833333333334\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gc\n",
    "\n",
    "# Set a seed value.\n",
    "seed_val = 1024\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the accuracy scores for each fold model in this list.\n",
    "# [[model_0 scores], [model_1 scores], [model_2 scores], [model_3 scores], [model_4 scores]]\n",
    "# [[ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...]]\n",
    "\n",
    "# Create a list of lists to store the val acc results.\n",
    "# The number of items in this list will correspond to\n",
    "# the number of folds that the model is being trained on.\n",
    "fold_val_acc_list = []\n",
    "for i in range(0, NUM_FOLDS):\n",
    "    \n",
    "    # append an empty list\n",
    "    fold_val_acc_list.append([])\n",
    "    \n",
    "# For each epoch...\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    \n",
    "    print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
    "    \n",
    "    # Get the number of folds\n",
    "    num_folds = len(train_df)\n",
    "\n",
    "    # For this epoch, store the val acc scores for each fold in this list.\n",
    "    # We will use this list to calculate the cv at the end of the epoch.\n",
    "    epoch_acc_scores_list = []\n",
    "    \n",
    "    # For each fold...\n",
    "    for fold_index in range(0, NUM_FOLDS_TO_TRAIN):\n",
    "        \n",
    "        print('\\n== Fold Model', fold_index)\n",
    "        \n",
    "        # .........................\n",
    "        # Load the fold model\n",
    "        # .........................\n",
    "        \n",
    "        if epoch == 0:\n",
    "            # define the model\n",
    "            model = BertForSequenceClassification.from_pretrained(\n",
    "                MODEL_TYPE, \n",
    "                num_labels = 3,       \n",
    "                output_attentions = False, \n",
    "                output_hidden_states = False,\n",
    "            )\n",
    "            \n",
    "            # Send the model to the device.\n",
    "            model.to(device)\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(model.parameters(),\n",
    "              lr = L_RATE, \n",
    "              eps = 1e-8\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            # Get the fold model\n",
    "            path_model = 'model_' + str(fold_index) + '.bin'\n",
    "            model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "            # Send the model to the device.\n",
    "            model.to(device)\n",
    "        \n",
    "        # .....................................\n",
    "        # Set up the train and val dataloaders\n",
    "        # .....................................\n",
    "        \n",
    "        \n",
    "        # Intialize the fold dataframes\n",
    "        # df_train = train_df[fold_index]\n",
    "        # df_val = val_df_list[fold_index]\n",
    "        \n",
    "        # Reset the indices or the dataloader won't work.\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "        df_val = df_val.reset_index(drop=True)\n",
    "    \n",
    "        # Create the dataloaders\n",
    "        train_data = CompDataset(df_train)\n",
    "        val_data = CompDataset(df_val)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                               num_workers=0)\n",
    "\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_data,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        stacked_val_labels = []\n",
    "        targets_list = []\n",
    "\n",
    "        print('Training...')\n",
    "\n",
    "        # put the model into train mode\n",
    "        model.train()\n",
    "\n",
    "        # This turns gradient calculations on and off.\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n",
    "\n",
    "            print(train_status, end='\\r')\n",
    "\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_token_type_ids = batch[2].to(device)\n",
    "            b_labels = batch[3].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=b_token_type_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Use the optimizer to update Weights\n",
    "            \n",
    "            # Optimizer for GPU\n",
    "            optimizer.step() \n",
    "            \n",
    "            # Optimizer for TPU\n",
    "            # https://pytorch.org/xla/\n",
    "            # xm.optimizer_step(optimizer, barrier=True)\n",
    "            \n",
    "        print('Train loss:' ,total_train_loss)\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "\n",
    "        print('\\nValidation...')\n",
    "\n",
    "        # Put the model in evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        # Turn off the gradient calculations.\n",
    "        # This tells the model not to compute or store gradients.\n",
    "        # This step saves memory and speeds up validation.\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for j, val_batch in enumerate(val_dataloader):\n",
    "\n",
    "            val_status = 'Batch ' + str(j+1) + ' of ' + str(len(val_dataloader))\n",
    "\n",
    "            print(val_status, end='\\r')\n",
    "\n",
    "            b_input_ids = val_batch[0].to(device)\n",
    "            b_input_mask = val_batch[1].to(device)\n",
    "            b_token_type_ids = val_batch[2].to(device)\n",
    "            b_labels = val_batch[3].to(device)      \n",
    "\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                    token_type_ids=b_token_type_ids, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_val_loss = total_val_loss + loss.item()\n",
    "\n",
    "            # Get the preds\n",
    "            preds = outputs[1]\n",
    "\n",
    "            # Move preds to the CPU\n",
    "            val_preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Move the labels to the cpu\n",
    "            targets_np = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Append the labels to a numpy list\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_val_preds = val_preds\n",
    "\n",
    "            else:\n",
    "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "                \n",
    "                \n",
    "                \n",
    "        # .........................................\n",
    "        # Calculate the val accuracy for this fold\n",
    "        # .........................................      \n",
    "\n",
    "\n",
    "        # Calculate the validation accuracy\n",
    "        y_true = targets_list\n",
    "        y_pred = np.argmax(stacked_val_preds, axis=1)\n",
    "\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        epoch_acc_scores_list.append(val_acc)\n",
    "\n",
    "        print('Val loss:' ,total_val_loss)\n",
    "        print('Val acc: ', val_acc)\n",
    "        \n",
    "        # .........................\n",
    "        # Save the best model\n",
    "        # .........................\n",
    "        \n",
    "        if epoch == 0:\n",
    "            # Save the Model\n",
    "            model_name = 'model_' + str(fold_index) + '.bin'\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print('Saved model as ', model_name)\n",
    "            \n",
    "        if epoch != 0:\n",
    "            val_acc_list = fold_val_acc_list[fold_index]\n",
    "            best_val_acc = max(val_acc_list)\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                # save the model\n",
    "                model_name = 'model_' + str(fold_index) + '.bin'\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "                print('Val acc improved. Saved model as ', model_name)\n",
    "                \n",
    "                \n",
    "                \n",
    "        # .....................................\n",
    "        # Save the val_acc for this fold model\n",
    "        # .....................................\n",
    "        \n",
    "        # Note: Don't do this before the above 'Save Model' code or \n",
    "        # the save model code won't work. This is because the best_val_acc will\n",
    "        # become current val accuracy.\n",
    "                \n",
    "        # fold_val_acc_list is a list of lists.\n",
    "        # Each fold model has it's own list corresponding to the fold index.\n",
    "        # Here we choose a list corresponding to the fold number and append the acc score to that list.\n",
    "        fold_val_acc_list[fold_index].append(val_acc)\n",
    "        \n",
    "        # Use the garbage collector to save memory.\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    # .............................................................\n",
    "    # Calculate the CV accuracy score over all folds in this epoch\n",
    "    # .............................................................   \n",
    "        \n",
    "        \n",
    "    # Print the average val accuracy for all 5 folds\n",
    "    cv_acc = sum(epoch_acc_scores_list)/NUM_FOLDS_TO_TRAIN\n",
    "    print(\"\\nCV Acc:\", cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a89fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.81405], [0.80035], [0.81315], [], []]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the accuracy scores for each fold model.  \n",
    "# For info: \n",
    "# Fold model 0 is only training on fold 0 in each epoch.\n",
    "# The same applies to the other fold models.\n",
    "\n",
    "fold_val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "908e1515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9796"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataloader\n",
    "\n",
    "test_data = TestDataset(df_test)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37982059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2449\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                       num_workers=0)\n",
    "\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1eea0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set...\n",
      "Total batches: 2449\n",
      "\n",
      "Fold Model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2449\n",
      "Fold Model 1\n",
      "Batch 2449\n",
      "Fold Model 2\n",
      "Batch 2449\n",
      "Prediction complete.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#               Test Set\n",
    "# ========================================\n",
    "\n",
    "print('\\nTest Set...')\n",
    "\n",
    "model_preds_list = []\n",
    "\n",
    "print('Total batches:', len(test_dataloader))\n",
    "\n",
    "for fold_index in range(0, NUM_FOLDS_TO_TRAIN):\n",
    "    \n",
    "    print('\\nFold Model', fold_index)\n",
    "\n",
    "    # Load the fold model\n",
    "    path_model = 'model_' + str(fold_index) + '.bin'\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "    # Send the model to the device.\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    stacked_val_labels = []\n",
    "    \n",
    "\n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Turn off the gradient calculations.\n",
    "    # This tells the model not to compute or store gradients.\n",
    "    # This step saves memory and speeds up validation.\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_val_loss = 0\n",
    "\n",
    "    for j, h_batch in enumerate(test_dataloader):\n",
    "\n",
    "        inference_status = 'Batch ' + str(j + 1)\n",
    "\n",
    "        print(inference_status, end='\\r')\n",
    "\n",
    "        b_input_ids = h_batch[0].to(device)\n",
    "        b_input_mask = h_batch[1].to(device)\n",
    "        b_token_type_ids = h_batch[2].to(device)     \n",
    "\n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                token_type_ids=b_token_type_ids, \n",
    "                attention_mask=b_input_mask)\n",
    "\n",
    "\n",
    "        # Get the preds\n",
    "        preds = outputs[0]\n",
    "\n",
    "\n",
    "        # Move preds to the CPU\n",
    "        val_preds = preds.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        # Stack the predictions.\n",
    "\n",
    "        if j == 0:  # first batch\n",
    "            stacked_val_preds = val_preds\n",
    "\n",
    "        else:\n",
    "            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "\n",
    "        \n",
    "    model_preds_list.append(stacked_val_preds)\n",
    "    \n",
    "            \n",
    "print('\\nPrediction complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "02e0cef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.3344262 ,  0.6812976 ,  1.9206414 ],\n",
       "        [-1.8758273 , -1.5486668 ,  3.872561  ],\n",
       "        [-1.9655329 , -1.645029  ,  3.691682  ],\n",
       "        ...,\n",
       "        [ 4.492677  , -0.90280604, -3.264653  ],\n",
       "        [-1.0716397 ,  4.316246  , -3.094708  ],\n",
       "        [-0.99402064, -0.7566957 ,  2.2035198 ]], dtype=float32),\n",
       " array([[-1.8577287 ,  1.8884472 , -0.35532168],\n",
       "        [-2.1804807 , -0.6227617 ,  3.28483   ],\n",
       "        [-1.5078266 , -1.1880586 ,  3.0865061 ],\n",
       "        ...,\n",
       "        [ 1.9394854 ,  0.17191732, -2.4049466 ],\n",
       "        [-1.4307463 ,  3.7661881 , -2.6184115 ],\n",
       "        [ 0.05966856, -0.23168522, -0.12326422]], dtype=float32),\n",
       " array([[-3.527332  ,  2.351716  ,  0.957407  ],\n",
       "        [-2.091532  ,  0.78125805,  1.7542137 ],\n",
       "        [-2.6522515 ,  0.9257445 ,  2.2227507 ],\n",
       "        ...,\n",
       "        [ 3.1736846 ,  0.15085834, -3.5222862 ],\n",
       "        [-0.84656435,  3.9485471 , -3.6424134 ],\n",
       "        [-0.72639185,  0.14315908,  1.3065228 ]], dtype=float32)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47aaf3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the predictions of all fold models\n",
    "for i, item in enumerate(model_preds_list):\n",
    "    if i == 0:\n",
    "        preds = item\n",
    "    else:\n",
    "        # Sum the matrices\n",
    "        preds = item + preds\n",
    "        \n",
    "# Average the predictions\n",
    "avg_preds = preds/(len(model_preds_list))\n",
    "\n",
    "test_preds = np.argmax(avg_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb0446d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, ..., 0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d149eec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairID</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9848</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9849</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pairID  gold_label\n",
       "0    9847           1\n",
       "1    9848           2\n",
       "2    9849           2\n",
       "3    9850           1\n",
       "4    9851           0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'pairID': df_test['pairID'], 'gold_label': test_preds}\n",
    "\n",
    "submission_df = pd.DataFrame(data)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08d40ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairID</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9847</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9848</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9849</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9850</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9851</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pairID     gold_label\n",
       "0    9847        neutral\n",
       "1    9848     entailment\n",
       "2    9849     entailment\n",
       "3    9850        neutral\n",
       "4    9851  contradiction"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_label(code):\n",
    "    if code == 1:\n",
    "        return \"neutral\"\n",
    "    elif code == 0:\n",
    "        return \"contradiction\"\n",
    "    elif code == 2:\n",
    "        return \"entailment\"\n",
    "    \n",
    "submission_df[\"gold_label\"] = submission_df[\"gold_label\"].apply(lambda x: convert_to_label(x))\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8bc00cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9796"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6cb2d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('bert_mnli_kaggle_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c27355",
   "metadata": {},
   "source": [
    "<h3>My submission retrained on BERT scored 83.738% meaning that I need to alter or add features to improve the score.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890450e0",
   "metadata": {},
   "source": [
    "![title](bert_retrained_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc8ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b42bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaf66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
