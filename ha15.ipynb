{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b41811",
   "metadata": {},
   "source": [
    "<b>Chris Huber\\\n",
    "CSC820, Prof Anagha Kulkarni\\\n",
    "Spring 2022\\\n",
    "HA15\\\n",
    "ha15.ipynb</b>\n",
    "\n",
    "<b>Description</b>: This notebook implements a lesson posted at https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/. We manually create train and test datasets based on a prefix contained in the filenames which splits it into a train set of 58109 and a test set of 6,611 sentences. I then create an embedding using Word2Vec and the vocabulary from the corpus. The Keras CNN model uses that embedding layer along with a convolutional and pooling layer which creates a movie review sentiment analysis predictor. I then use that model to evaluate on the test dataset for accuracy over 10 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42511de",
   "metadata": {},
   "source": [
    "<h3>Lesson 07: Movie Review Sentiment Analysis Project</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fb33801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b35a6",
   "metadata": {},
   "source": [
    "<h3>Define Utility Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22dd794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    " \n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    \n",
    "    print(tokens)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # split into tokens by white space\n",
    "        tokens = line.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # filter out tokens not in vocab\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972d075",
   "metadata": {},
   "source": [
    "<h3>Define Vocabulary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ada8a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "\n",
    "# add all docs to vocab\n",
    "process_docs('./txt_sentoken/pos', vocab)\n",
    "process_docs('./txt_sentoken/neg', vocab)\n",
    "\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea18c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurance = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbb2d5",
   "metadata": {},
   "source": [
    "<h3>Create Vocabulary and Export as Text File</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b89cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open|\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # add lines to list\n",
    "        lines += doc_lines\n",
    "    return lines\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    \n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    \n",
    "    # write text\n",
    "    file.write(data)\n",
    "    \n",
    "    # close file\n",
    "    file.close()\n",
    " \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67623245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative docs: 28584\n",
      "Number of positive docs: 29525\n",
      "Total training sentences: 58109\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load training data\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
    "print('Number of negative docs: %d' % len(negative_docs))\n",
    "print('Number of positive docs: %d' % len(positive_docs))\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b256ad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25767\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "# words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(model.wv))\n",
    " \n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b654fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58109"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create concatenated train docs with negative reviews first, then positive\n",
    "train_docs = negative_docs + positive_docs\n",
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6beccb",
   "metadata": {},
   "source": [
    "<h3>Create Keras Tokenizer Fit Using Newly Created Embedding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3025874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da29343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8626fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pad sequences\n",
    "max_length = max([len(s) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "# generate training labels for 58109 sentences using counts from prelabelled sets\n",
    "ytrain = array([0 for _ in range(28584)] + [1 for _ in range(29525)])\n",
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "362dcb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative docs: 3199\n",
      "Number of positive docs: 3412\n"
     ]
    }
   ],
   "source": [
    "# load selected test reviews and get counts to assign labels\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, False)\n",
    "print('Number of negative docs: %d' % len(negative_docs))\n",
    "print('Number of positive docs: %d' % len(positive_docs))\n",
    "test_docs = negative_docs + positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d776089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(3199)] + [1 for _ in range(3412)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfc7b0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25768"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa034eb",
   "metadata": {},
   "source": [
    "<h3> Add Embedding Layer Created Using Exported Word2Vec Text File</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed903887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray, zeros\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c985bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 75, 100)           2576800   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 68, 32)            25632     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 34, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1088)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                10890     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,613,333\n",
      "Trainable params: 2,613,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c67621ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1816/1816 - 35s - loss: 0.6170 - accuracy: 0.6468 - 35s/epoch - 19ms/step\n",
      "Epoch 2/10\n",
      "1816/1816 - 35s - loss: 0.4241 - accuracy: 0.7998 - 35s/epoch - 19ms/step\n",
      "Epoch 3/10\n",
      "1816/1816 - 37s - loss: 0.2042 - accuracy: 0.9107 - 37s/epoch - 20ms/step\n",
      "Epoch 4/10\n",
      "1816/1816 - 36s - loss: 0.1009 - accuracy: 0.9555 - 36s/epoch - 20ms/step\n",
      "Epoch 5/10\n",
      "1816/1816 - 36s - loss: 0.0629 - accuracy: 0.9721 - 36s/epoch - 20ms/step\n",
      "Epoch 6/10\n",
      "1816/1816 - 37s - loss: 0.0476 - accuracy: 0.9782 - 37s/epoch - 20ms/step\n",
      "Epoch 7/10\n",
      "1816/1816 - 36s - loss: 0.0394 - accuracy: 0.9822 - 36s/epoch - 20ms/step\n",
      "Epoch 8/10\n",
      "1816/1816 - 36s - loss: 0.0368 - accuracy: 0.9836 - 36s/epoch - 20ms/step\n",
      "Epoch 9/10\n",
      "1816/1816 - 37s - loss: 0.0342 - accuracy: 0.9849 - 37s/epoch - 20ms/step\n",
      "Epoch 10/10\n",
      "1816/1816 - 37s - loss: 0.0313 - accuracy: 0.9859 - 37s/epoch - 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x232293a5100>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89eee182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.911964\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6220c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31037a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
